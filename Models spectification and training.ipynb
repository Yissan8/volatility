{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yissan8/volatility/blob/main/Models%20spectification%20and%20training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we create classes for the architectures of all our models, starting with the transformer architecture with its positional encoding function"
      ],
      "metadata": {
        "id": "bHlMuOdwK0n8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8hmEbQqzCYOl"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "from torch import nn, Tensor\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def position_encoding_init(n_position, d_pos_vec):\n",
        "    position_enc = np.array([\n",
        "        [math.pi*(pos/(n_position-1)) for i in range(d_pos_vec)]\n",
        "        if pos != 0 else np.zeros(d_pos_vec) for pos in range(n_position)])\n",
        "    return np.cos(position_enc)\n",
        "\n",
        "\n",
        "class customtransformer(nn.Module):\n",
        "\n",
        "    def __init__(self,dim = 256, inp = 32,n_heads = 8,drop = 0.1,drop_att = 0.1):\n",
        "\n",
        "        self.drop_att = drop_att\n",
        "        self.d = drop\n",
        "        self.n_heads = n_heads\n",
        "        self.inp = inp\n",
        "        self.dim = dim\n",
        "        super().__init__()\n",
        "\n",
        "        self.attention_query = nn.Linear(\n",
        "            in_features=dim,\n",
        "            out_features=dim\n",
        "            )\n",
        "\n",
        "        self.drop1 = nn.Dropout(p=self.d)\n",
        "        self.drop2 = nn.Dropout(p=self.d)\n",
        "\n",
        "        self.attention_value = nn.Linear(\n",
        "            in_features=dim,\n",
        "            out_features=dim\n",
        "            )\n",
        "\n",
        "        self.attention_key = nn.Linear(\n",
        "            in_features=dim,\n",
        "            out_features=dim\n",
        "            )\n",
        "\n",
        "\n",
        "        self.layernorm1 = nn.LayerNorm([inp, dim])\n",
        "        self.layernorm2 = nn.LayerNorm([inp,1])\n",
        "        self.layernorm3 = nn.LayerNorm(1)\n",
        "\n",
        "        self.drop = nn.Dropout(self.d)\n",
        "\n",
        "\n",
        "        self.linear_mapping_initial = nn.Linear(\n",
        "            in_features=1,\n",
        "            out_features=dim\n",
        "            )\n",
        "\n",
        "        self.linear_mapping1 = nn.Linear(\n",
        "            in_features=dim,\n",
        "            out_features=1\n",
        "            )\n",
        "\n",
        "        self.linear_mapping2 = nn.Linear(\n",
        "            in_features=inp,\n",
        "            out_features=1\n",
        "            )\n",
        "\n",
        "\n",
        "        self.r = nn.ReLU()\n",
        "        self.r2 = nn.ReLU()\n",
        "        self.sig = nn.Sigmoid()\n",
        "        self.sig2 = nn.Sigmoid()\n",
        "\n",
        "        self.ly1 = nn.LayerNorm([inp])\n",
        "        self.ly2 = nn.LayerNorm([inp,dim])\n",
        "\n",
        "        self.att = nn.MultiheadAttention(embed_dim = dim,num_heads=n_heads,dropout=self.drop_att)\n",
        "\n",
        "\n",
        "    def forward(self, src: Tensor) -> Tensor:\n",
        "\n",
        "        t = torch.Tensor(position_encoding_init(self.inp,1)).squeeze().to('cuda')\n",
        "        src = src + t\n",
        "        src_ = src# self.ly1(src)\n",
        "        src = self.linear_mapping_initial(src.unsqueeze(1))#.unsqueeze(0)\n",
        "\n",
        "        #src_a = torch.transpose()\n",
        "        q = self.attention_query(src)\n",
        "        k = self.attention_key(src)\n",
        "        v = self.attention_value(src)\n",
        "\n",
        "\n",
        "\n",
        "        attn_output, attn_output_weights = self.att(q,k,v)\n",
        "\n",
        "        x = self.drop1(attn_output)#.squeeze()\n",
        "\n",
        "\n",
        "        #x = self.layernorm1(src+x)\n",
        "        x = src + x #self.ly2(x)\n",
        "        x = self.linear_mapping1(x)\n",
        "        x = self.sig(x)\n",
        "        x = self.drop2(x)\n",
        "        x = src_ + x.squeeze()\n",
        "        x = self.linear_mapping2(x)\n",
        "        #x = self.r(x)\n",
        "        #x = self.layernorm3(x)\n",
        "        #x = self.sig(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we create a class and functions that will allow us to process a 1-D tensor inpput into a shape that is digestible by our transformer"
      ],
      "metadata": {
        "id": "5_zK5QN5MAIe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FhTGJsstCbWY"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import numpy as np\n",
        "from torch import nn, Tensor\n",
        "from typing import Optional, Any, Union, Callable, Tuple\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "class TransformerDataset(Dataset):\n",
        "    def __init__(self,\n",
        "        data: torch.tensor,\n",
        "        indices: list,\n",
        "        enc_seq_len: int,\n",
        "        dec_seq_len: int,\n",
        "        target_seq_len: int,\n",
        "        wind,\n",
        "        mean,\n",
        "        std\n",
        "        ) -> None:\n",
        "\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.indices = indices\n",
        "\n",
        "        self.data = data\n",
        "\n",
        "        print(\"From get_src_trg: data size = {}\".format(data.size()))\n",
        "\n",
        "        self.enc_seq_len = enc_seq_len\n",
        "\n",
        "        self.dec_seq_len = dec_seq_len\n",
        "\n",
        "        self.target_seq_len = target_seq_len\n",
        "\n",
        "\n",
        "        self.wind = wind\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "\n",
        "    def __len__(self):\n",
        "\n",
        "        return len(self.indices)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        start_idx = self.indices[index][0]\n",
        "        end_idx = self.indices[index][1]\n",
        "\n",
        "        sequence = self.data[start_idx:end_idx]\n",
        "\n",
        "        src, trg, trg_y = self.get_src_trg(\n",
        "            sequence=sequence,\n",
        "            enc_seq_len=self.enc_seq_len,\n",
        "            dec_seq_len=self.dec_seq_len,\n",
        "            target_seq_len=self.target_seq_len,\n",
        "            wind = self.wind,\n",
        "            mean = self.mean,\n",
        "            std = self.std\n",
        "            )\n",
        "\n",
        "        return src, trg, trg_y\n",
        "\n",
        "    def get_src_trg(\n",
        "        self,\n",
        "        sequence: torch.Tensor,\n",
        "        enc_seq_len: int,\n",
        "        dec_seq_len: int,\n",
        "        target_seq_len: int,\n",
        "        wind,\n",
        "        mean,\n",
        "        std\n",
        "        ) -> Tuple[torch.tensor, torch.tensor, torch.tensor]:\n",
        "\n",
        "        src = (sequence[:enc_seq_len] - mean)/std\n",
        "\n",
        "        trg = sequence[enc_seq_len-1:len(sequence)-1-wind]\n",
        "\n",
        "        trg_y = sequence[-target_seq_len:]\n",
        "        return src, trg, trg_y.squeeze(-1)\n",
        "\n",
        "def generate_square_subsequent_mask(dim1: int, dim2: int) -> Tensor:\n",
        "\n",
        "    return torch.triu(torch.ones(dim1, dim2) * float('-inf'), diagonal=1)\n",
        "\n",
        "\n",
        "def get_indices_input_target(num_obs, input_len, step_size, forecast_horizon, target_len,wind:int):\n",
        "\n",
        "        input_len = round(input_len) # just a precaution\n",
        "        start_position = 0\n",
        "        stop_position = num_obs # because of 0 indexing\n",
        "\n",
        "        subseq_first_idx = start_position\n",
        "        subseq_last_idx = start_position + input_len\n",
        "        target_first_idx = subseq_last_idx + forecast_horizon\n",
        "        target_last_idx = target_first_idx + target_len\n",
        "        print(\"target_last_idx is {}\".format(target_last_idx))\n",
        "        print(\"stop_position is {}\".format(stop_position))\n",
        "        indices = []\n",
        "        while target_last_idx <= stop_position:\n",
        "            indices.append((subseq_first_idx, subseq_last_idx, target_first_idx, target_last_idx))\n",
        "            subseq_first_idx += step_size\n",
        "            subseq_last_idx += step_size\n",
        "            target_first_idx = subseq_last_idx + forecast_horizon + wind\n",
        "            target_last_idx = target_first_idx + target_len + wind\n",
        "\n",
        "        return indices\n",
        "\n",
        "def get_indices_entire_sequence(data: pd.DataFrame, window_size: int, step_size: int,wind:int) -> list:\n",
        "\n",
        "        stop_position = len(data) # 1- because of 0 indexing\n",
        "\n",
        "        # Start the first sub-sequence at index position 0\n",
        "        subseq_first_idx = 0\n",
        "\n",
        "        subseq_last_idx = window_size +wind\n",
        "\n",
        "        indices = []\n",
        "\n",
        "        while subseq_last_idx <= stop_position:\n",
        "\n",
        "            indices.append((subseq_first_idx, subseq_last_idx))\n",
        "\n",
        "            subseq_first_idx += step_size\n",
        "\n",
        "            subseq_last_idx += step_size\n",
        "\n",
        "        return indices\n",
        "\n",
        "\n",
        "def read_data(data_path,\n",
        "    timestamp_col_name: str=\"Date\") -> pd.DataFrame:\n",
        "\n",
        "    print(\"Reading file in {}\".format(data_path))\n",
        "\n",
        "    data = pd.read_csv(\n",
        "        data_path,\n",
        "        parse_dates=[timestamp_col_name],\n",
        "        index_col=[timestamp_col_name],\n",
        "        infer_datetime_format=True,\n",
        "        low_memory=False\n",
        "    )\n",
        "\n",
        "\n",
        "    if is_ne_in_df(data):\n",
        "        raise ValueError(\"data frame contains 'n/e' values. These must be handled\")\n",
        "\n",
        "    data = to_numeric_and_downcast_data(data)\n",
        "\n",
        "    data.sort_values(by=[timestamp_col_name], inplace=True)\n",
        "\n",
        "    return data\n",
        "\n",
        "def is_ne_in_df(df:pd.DataFrame):\n",
        "    for col in df.columns:\n",
        "\n",
        "        true_bool = (df[col] == \"n/e\")\n",
        "\n",
        "        if any(true_bool):\n",
        "            return True\n",
        "\n",
        "    return False\n",
        "\n",
        "\n",
        "def to_numeric_and_downcast_data(df: pd.DataFrame):\n",
        "    fcols = df.select_dtypes('float').columns\n",
        "\n",
        "    icols = df.select_dtypes('integer').columns\n",
        "\n",
        "    df[fcols] = df[fcols].apply(pd.to_numeric, downcast='float')\n",
        "\n",
        "    df[icols] = df[icols].apply(pd.to_numeric, downcast='integer')\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "huy0FrA_kt5p"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "import datetime\n",
        "import numpy as np\n",
        "\n",
        "def trans_data(data,horizon,enc_seq_len = 30,output_sequence_length = 1,step_size=1,dec_seq_len = 1):\n",
        "  data = data.iloc[:,0]\n",
        "  mean = data.mean()\n",
        "  std = data.std()\n",
        "  window_size = enc_seq_len + output_sequence_length\n",
        "  training_data = pd.DataFrame(data[:round(0.85*len(data))])\n",
        "\n",
        "  validation_data = pd.DataFrame(data[round(0.85*len(data)):])\n",
        "\n",
        "  dates = validation_data.index\n",
        "\n",
        "\n",
        "  training_indices = get_indices_entire_sequence(\n",
        "      data=training_data,\n",
        "      window_size=window_size,\n",
        "      step_size=step_size,  wind = horizon -1)\n",
        "\n",
        "  validation_indices = get_indices_entire_sequence(\n",
        "      data=validation_data,\n",
        "      window_size=window_size,\n",
        "      step_size=step_size, wind = horizon - 1)\n",
        "\n",
        "\n",
        "  training_data = TransformerDataset(\n",
        "      data=torch.tensor(training_data.iloc[:,0].values).float(),\n",
        "      indices=training_indices,\n",
        "      enc_seq_len=enc_seq_len,\n",
        "      dec_seq_len=dec_seq_len,\n",
        "      target_seq_len=output_sequence_length,\n",
        "      mean = mean,\n",
        "      std = std,\n",
        "      wind = horizon - 1\n",
        "      )\n",
        "\n",
        "  validation_data = TransformerDataset(\n",
        "      data=torch.tensor(validation_data.iloc[:,0].values).float(),\n",
        "      indices=validation_indices,\n",
        "      enc_seq_len=enc_seq_len,\n",
        "      dec_seq_len=dec_seq_len,\n",
        "      target_seq_len=output_sequence_length,\n",
        "      mean = mean,\n",
        "      std = std,\n",
        "      wind = horizon - 1\n",
        "      )\n",
        "\n",
        "  training = []\n",
        "  for j in training_data:\n",
        "    r,t,y = (enumerate(j))\n",
        "    d = [r,y]\n",
        "    training.append(d)\n",
        "\n",
        "  validation = []\n",
        "\n",
        "  for j in validation_data:\n",
        "    r,t,y = (enumerate(j))\n",
        "    d = [r,y]\n",
        "    validation.append(d)\n",
        "\n",
        "  return training,validation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we create a funtion to process the 1-D timeseries into a digestible format for the LSTM"
      ],
      "metadata": {
        "id": "nNNJrJuaZ46s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ms_VqnmWhtJb"
      },
      "outputs": [],
      "source": [
        "def create_dataset(dataset, lookback,ahead):\n",
        "    X, y, ind = [], [], []\n",
        "    for i in range(lookback,len(dataset)-ahead):\n",
        "        feature = dataset[i-lookback:i]\n",
        "        target = dataset[i+ahead]\n",
        "        X.append(feature)\n",
        "        y.append(target)\n",
        "        #ind.append(dataset.index[i+ahead])\n",
        "    return torch.tensor(X), torch.tensor(y)#,dataset[ahead:]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we create the function to process a 1-D timeseries into a digestible format for the Boosting models"
      ],
      "metadata": {
        "id": "64BDwXo9aKtb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PrbWv3aUCi0P"
      },
      "outputs": [],
      "source": [
        "from pandas import DataFrame\n",
        "from pandas import concat\n",
        "\n",
        "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
        "\tn_out = n_out+1\n",
        "\tn_vars = 1 if type(data) is list else data.shape[1]\n",
        "\tdf = DataFrame(data)\n",
        "\tcols, names = list(), list()\n",
        "\tfor i in range(n_in, 0, -1):\n",
        "\t\tcols.append(df.shift(i))\n",
        "\t\tnames += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
        "\tfor i in range(0, n_out):\n",
        "\t\tcols.append(df.shift(-i))\n",
        "\t\tif i == 0:\n",
        "\t\t\tnames += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
        "\t\telse:\n",
        "\t\t\tnames += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
        "\t# put it all together\n",
        "\tagg = concat(cols, axis=1)\n",
        "\tagg.columns = names\n",
        "\t# drop rows with NaN values\n",
        "\tif dropnan:\n",
        "\t\tagg.dropna(inplace=True)\n",
        "\treturn agg\n",
        "\n",
        "def create_sequence(dataset, lookback,ahead):\n",
        "\tX = []\n",
        "\tmean = dataset.mean()\n",
        "\tstd = dataset.std()\n",
        "\tdataset = (dataset -mean) /std\n",
        "\tfor i in range(lookback,len(dataset)-ahead):\n",
        "\t\t\tfeature = torch.from_numpy(np.array(dataset[i-lookback:i]))\n",
        "\t\t\tX.append(feature)\n",
        "\treturn X"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we create the function for importing historical daily prices for any stock ticker and converting them into returns and volatiliity time series"
      ],
      "metadata": {
        "id": "TMIYOQ7oYdvy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TOSbpq_glWx_"
      },
      "outputs": [],
      "source": [
        "import yfinance as yf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def ds(Ticker,output='vol'):\n",
        "  SP = yf.Ticker(Ticker)\n",
        "  SP = SP.history(start='1989-12-22').reset_index()\n",
        "  SP['SP_returns'] = np.log(SP['Close']/SP['Close'].shift(1))\n",
        "  SP['Volatility'] = SP['SP_returns'].rolling(5).std()\n",
        "  SP['Date'] = pd.to_datetime(SP['Date']).dt.date\n",
        "  if output == 'vol':\n",
        "    dataset = SP[['Date','Volatility']].dropna().set_index('Date')\n",
        "    dataset.columns = [Ticker + '_volatility']\n",
        "  elif output == 'return':\n",
        "    dataset = SP[['Date','SP_returns']].dropna().set_index('Date')\n",
        "    dataset.columns = [Ticker + '_returns']\n",
        "  return dataset\n",
        "\n",
        "\n",
        "\n",
        "SP = ds(\"^GSPC\")\n",
        "NQ = ds(\"^IXIC\")\n",
        "RTY = ds(\"^RUT\")\n",
        "COM = ds(\"^DJCI\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we create the functions to evaluate and train our transformer"
      ],
      "metadata": {
        "id": "PeaDhEN2bQO_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "efxBVMKHEAtt"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "def train(model: nn.Module,criterion,optimizer,training) -> None:\n",
        "    model.train()  # turn on train mode\n",
        "    total_loss = 0.\n",
        "    y = torch.Tensor().cuda()\n",
        "    out = torch.Tensor().cuda()\n",
        "\n",
        "    log_interval = 200\n",
        "    start_time = time.time()\n",
        "    for i in range(len(training)):\n",
        "      src = training[i][0][1]\n",
        "      src = src.cuda()\n",
        "      trg_y = training[i][1][1]\n",
        "      trg_y = trg_y.cuda()\n",
        "      output = model(\n",
        "      src=src\n",
        "      ).cuda()\n",
        "\n",
        "      y = torch.cat((y.cpu(),trg_y.unsqueeze(0).cpu()),0).cuda()\n",
        "      out = torch.cat((out.cuda(),output),0)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      if i%256 == 0 and i != 0:\n",
        "        total_loss = criterion(y,out) #+ criterion2(y,out)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        total_loss.backward()\n",
        "        #torch.nn.utils.clip_grad_norm_(model.parameters(), 40)\n",
        "        optimizer.step()\n",
        "        y = torch.Tensor()\n",
        "        out = torch.Tensor()\n",
        "\n",
        "\n",
        "def evaluate(model: nn.Module,criterion,validation) :\n",
        "    model.eval()\n",
        "    total_loss = 0.\n",
        "\n",
        "    with torch.no_grad():\n",
        "      y = torch.Tensor()\n",
        "      out = torch.Tensor()\n",
        "      for i in range(len(validation)):\n",
        "        src = validation[i][0][1].cuda()\n",
        "        trg_y = validation[i][1][1].cuda()\n",
        "        output = model(\n",
        "        src=src).cuda()\n",
        "        y = torch.cat((y.cpu(),trg_y.unsqueeze(0).cpu()),0).cuda()\n",
        "        out = torch.cat((out.cuda(),output),0).cuda()\n",
        "\n",
        "      total_loss = criterion(y,out)\n",
        "\n",
        "      print(f\"validation_loss: {total_loss:>7f},   batch n : {i:>7f}\")\n",
        "    return total_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is our LSTM model class"
      ],
      "metadata": {
        "id": "3vpEwhlpbeY_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ep5p3AYYENJO"
      },
      "outputs": [],
      "source": [
        "class LSTM(nn.Module):\n",
        "    def __init__(self, num_classes, input_size, hidden_size, num_layers, seq_length):\n",
        "        super(LSTM, self).__init__()\n",
        "        self.num_classes = num_classes #number of classes\n",
        "        self.num_layers = num_layers #number of layers\n",
        "        self.input_size = input_size #input size\n",
        "        self.hidden_size = hidden_size #hidden state\n",
        "        self.seq_length = seq_length #sequence length\n",
        "\n",
        "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size,\n",
        "                          num_layers=num_layers, batch_first=True) #lstm\n",
        "        self.fc_1 =  nn.Linear(hidden_size, 128) #fully connected 1\n",
        "        self.fc = nn.Linear(128, num_classes) #fully connected last layer\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self,x):\n",
        "        h_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size)) #hidden state\n",
        "        c_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size)) #internal state\n",
        "        # Propagate input through LSTM\n",
        "        output, (hn, cn) = self.lstm(x, (h_0, c_0)) #lstm with input, hidden, and internal state\n",
        "        hn = hn.view(-1, self.hidden_size) #reshaping the data for Dense layer next\n",
        "        out = self.relu(hn)\n",
        "        out = self.fc_1(out) #first Dense\n",
        "        out = self.relu(out) #relu\n",
        "        out = self.fc(out) #Final Output\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EWvveOCr_Me3"
      },
      "outputs": [],
      "source": [
        "!pip install ngboost"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is the single function used to train all 4 of our models. The function provides us with the flexibility to directly specify the ticker of the asset we want to train our models on as weel as the desired forecasting horizon"
      ],
      "metadata": {
        "id": "4cbeTXaUbp6A"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IjxcOiPejmcF"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from ngboost import NGBRegressor,NGBSurvival\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "import xgboost as xgb\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "\n",
        "\n",
        "\n",
        "def train_models(n,ticker='^GSPC',epochs=30,l_epochs=30):\n",
        "  data = ds(ticker)\n",
        "  cutoff = round(0.85*len(data))\n",
        "  training,validation = trans_data(data,n)\n",
        "  trans = customtransformer(dim = 128,inp = 30,n_heads = 8)\n",
        "  trans.cuda()\n",
        "  criterion = nn.L1Loss()\n",
        "  lr = 0.05\n",
        "  optimizer = torch.optim.Adam(trans.parameters(), lr=lr, weight_decay=1e-5)\n",
        "  scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.01, total_iters=90,verbose=True)\n",
        "  best_val_loss = 100000\n",
        "\n",
        "\n",
        "  for epoch in range(epochs + 1):\n",
        "    if epoch!=0:\n",
        "      trans.load_state_dict(torch.load('/content/transformer_params'))\n",
        "    epoch_start_time = time.time()\n",
        "    train(trans,criterion,optimizer,training)\n",
        "    val_loss = evaluate(trans,criterion, validation )\n",
        "    elapsed = time.time() - epoch_start_time\n",
        "    if val_loss < best_val_loss:\n",
        "      best_val_loss = val_loss\n",
        "      torch.save(trans.state_dict(), '/content/transformer_params')\n",
        "\n",
        "    scheduler.step()\n",
        "    print(f\"epoch n:  {epoch}\")\n",
        "\n",
        "  ss = StandardScaler()\n",
        "  train_l = np.array(data.iloc[:cutoff,0].values.astype('float32'))\n",
        "  train_l = ss.fit_transform(train_l.reshape(len(train_l),1))\n",
        "  X_train, y_train = create_dataset(train_l,30,h - 1)\n",
        "  y_train_tensors = Variable(torch.Tensor(y_train))\n",
        "\n",
        "  lstm = LSTM(1, 1, 4, 1, 30)\n",
        "\n",
        "  lr = 0.1\n",
        "  criterion = nn.L1Loss()\n",
        "  optimizer = torch.optim.Adam(lstm.parameters(), lr=lr, weight_decay=1e-5)\n",
        "  scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.0001, total_iters=1750,verbose=True)\n",
        "  best_val_loss = 100000\n",
        "  for epoch in range(l_epochs):\n",
        "    if epoch!=0:\n",
        "      lstm.load_state_dict(torch.load('/content/lstm_params'))\n",
        "    outputs = lstm.forward(X_train)\n",
        "    optimizer.zero_grad()\n",
        "    loss = criterion(outputs, y_train_tensors)\n",
        "    loss.backward()\n",
        "    if loss < best_val_loss:\n",
        "      best_val_loss = loss\n",
        "      torch.save(lstm.state_dict(), '/content/lstm_params')\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "    print(\"Epoch: %d, loss: %1.5f\" % (epoch, loss.item()))\n",
        "\n",
        "\n",
        "  boost = series_to_supervised(data,10,n)\n",
        "  ng = NGBRegressor().fit(boost.iloc[:cutoff,:10], boost.iloc[:cutoff,-1])\n",
        "  xg = xgb.XGBRegressor(eval_metric='rmsle').fit(boost.iloc[:cutoff,:10], boost.iloc[:cutoff,-1])\n",
        "\n",
        "  #arch = arch_model(100*data,mean = 'AR',lags=1,dist='ged')\n",
        "  #arch.volatility = EGARCH(p=1, o=1, q=1)\n",
        "  #arch = arch.fit(last_obs=cutoff)\n",
        "\n",
        "  return xg,ng,lstm,trans#,arch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I55Z8ro8UrCQ"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xtdzdtj4dhLA"
      },
      "outputs": [],
      "source": [
        "import joblib\n",
        "folder_path = '/content/drive/MyDrive/Colab Notebooks/model_parameters'\n",
        "assets = [\"^GSPC\", \"^IXIC\", \"^RUT\",\"GC=F\",\"CL=F\"]\n",
        "#assets = [\"^RUT\"]\n",
        "for asset in assets:\n",
        "  for h in  [5,10,15,20,60]:25\n",
        "    mods = train_models(h,ticker = asset, epochs=125,l_epochs=2000)\n",
        "    joblib.dump(mods[0], folder_path + '/xgb_' + asset + '_' + str(h) + '_model')\n",
        "    joblib.dump(mods[1], folder_path + '/ngb_' + asset + '_' + str(h) + '_model')\n",
        "    joblib.dump(mods[2], folder_path + '/lstm_' + asset + '_' + str(h) + '_model')\n",
        "    joblib.dump(mods[3], folder_path + '/transformer_' + asset + '_' + str(h) + '_model')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ensemble(nn.Module):\n",
        "\n",
        "    def __init__(self,feat):\n",
        "\n",
        "        self.feat = feat\n",
        "        super().__init__()\n",
        "\n",
        "        self.map = nn.Linear(\n",
        "            in_features=feat,\n",
        "            out_features=1\n",
        "            )\n",
        "\n",
        "    def forward(self,x):\n",
        "        return self.map(x)"
      ],
      "metadata": {
        "id": "IOXbn_QilT54"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from torch.autograd import Variable\n",
        "def predict(models,x:pd.DataFrame,n=20):\n",
        "  g_data = series_to_supervised(pd.DataFrame(x),10,-1)\n",
        "  ng_res = pd.DataFrame(models[0].predict(g_data))\n",
        "  xg_res = pd.DataFrame(models[1].predict(g_data))\n",
        "  trans_data = create_sequence(x.reset_index(drop=True),30,n-1)\n",
        "  trans_res = []\n",
        "  for i in range(len(trans_data)):\n",
        "    trans_res.append(models[2](trans_data[i].squeeze().float().cuda()).squeeze(0).item())\n",
        "  trans_res= pd.DataFrame(trans_res)\n",
        "\n",
        "  #split_date\n",
        "  forecast = models[4].forecast(start=0,horizon = n,method='simulation')\n",
        "\n",
        "  G_forecast = forecast.variance/100\n",
        "  cutoff = round(len(x)*0.85)\n",
        "  #x = x.iloc[cutoff:]\n",
        "  G = pd.DataFrame(G_forecast.iloc[:-n,-1])\n",
        "  #G.index=dataset.index[cutoff+3:]\n",
        "  ss = StandardScaler()\n",
        "  mm = MinMaxScaler()\n",
        "\n",
        "  l_data = ss.fit_transform(np.array(x).reshape(len(x),1))\n",
        "  l_data, _ = create_dataset(l_data,30,n-1)\n",
        "\n",
        "  l_res = models[3](l_data.float()).detach().numpy()\n",
        "  #l_res = pd.DataFrame(l_res)\n",
        "  l_res = pd.DataFrame(ss.inverse_transform(l_res))\n",
        "  l_res.index = x.index[30+n-1:]\n",
        "\n",
        "  trans_res.index = x.index[30+n-1:]\n",
        "  ng_res.index = x.index[10:]\n",
        "  xg_res.index = x.index[10:]\n",
        "  ds = ng_res.merge(xg_res,left_index=True,right_index=True).merge(l_res,left_index=True,right_index=True).merge(trans_res,left_index=True,right_index=True).merge(G,left_index=True,right_index=True).merge(x,left_index=True,right_index=True)\n",
        "  ds.columns = ['NgBoost','XgBoost','LSTM','Transformer','GARCH','actual']\n",
        "  return ds\n",
        "\n",
        "#ds = predict([ng,regressor,trans,lstm,arch],data,n=20)"
      ],
      "metadata": {
        "id": "QxAFApzmDbwV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_ensemble(mod: nn.Module,optimizer,df) -> None:\n",
        "    mod.train()  # turn on train mode\n",
        "    total_loss = 0.\n",
        "    y = torch.Tensor()\n",
        "    out = torch.Tensor()\n",
        "    criterion = nn.L1Loss()\n",
        "    log_interval = 200\n",
        "    #start_time = time.time()\n",
        "    for i in range(len(df)):\n",
        "      src = torch.Tensor(df.iloc[i,:-1])\n",
        "      trg_y = torch.Tensor([df.iloc[i,-1]])\n",
        "      output = mod(src)\n",
        "\n",
        "      y = torch.cat([y,trg_y])\n",
        "      out = torch.cat([out,output])\n",
        "\n",
        "\n",
        "\n",
        "      total_loss = criterion(y,out)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      total_loss.backward()\n",
        "\n",
        "      optimizer.step()\n",
        "      y = torch.Tensor()\n",
        "      out = torch.Tensor()\n"
      ],
      "metadata": {
        "id": "kcld42WyneCA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install arch\n",
        "from arch.univariate import GARCH, ARCH, FIGARCH,EGARCH,APARCH, EWMAVariance\n",
        "from arch import arch_model"
      ],
      "metadata": {
        "id": "1BR5ZRfXqMf2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c9gUbEykPiZk"
      },
      "outputs": [],
      "source": [
        "import joblib\n",
        "def horizon_result(n,ticker='^GSPC',epochs=30):\n",
        "  data = ds(ticker)#['Volatility']\n",
        "  g_data = ds(ticker, 'return')\n",
        "  cutoff = round(0.85*len(data))\n",
        "  cutoff1 = round(0.35*len(data))\n",
        "  folder_path = '/content/drive/MyDrive/Colab Notebooks/model_parameters'\n",
        "  xg_path = folder_path + '/xgb_' + ticker + '_' + str(n) + '_model'\n",
        "  ng_path =  folder_path + '/ngb_' + ticker + '_' + str(n) + '_model'\n",
        "  lstm_path = folder_path + '/lstm_' + ticker + '_' + str(n) + '_model'\n",
        "  tr_path =  folder_path + '/transformer_' + ticker + '_' + str(n) + '_model'\n",
        "\n",
        "  arch = arch_model(100*data,mean = 'AR',lags=1,dist='ged')\n",
        "  arch.volatility = EGARCH(p=1, o=1, q=1)\n",
        "  arch = arch.fit(last_obs=cutoff)\n",
        "\n",
        "  ng = joblib.load(ng_path)\n",
        "  xg = joblib.load(xg_path)\n",
        "  lstm = joblib.load(lstm_path)\n",
        "  trans = joblib.load(tr_path)\n",
        "\n",
        "  td = predict([ng,xg,trans,lstm,arch],data,n=n)\n",
        "  ens = ensemble(5)\n",
        "  ens_s = ensemble(4)\n",
        "  criterion = nn.L1Loss()\n",
        "  lr = 0.01\n",
        "  optimizer = torch.optim.Adam(ens.parameters(), lr=lr, weight_decay=1e-5)\n",
        "  scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.4)\n",
        "  for epoch in range(50):\n",
        "    train_ensemble(ens,optimizer,td[round(0.35*len(td)):round(0.85*len(td))])\n",
        "    train_ensemble(ens_s,optimizer,td[['NgBoost','XgBoost','LSTM','Transformer','actual']][round(0.35*len(td)):round(0.85*len(td))])\n",
        "    scheduler.step()\n",
        "    print(f\"epoch n:  {epoch}\")\n",
        "\n",
        "  Ens_pred = []\n",
        "  test = td[round(0.85*len(td)):]\n",
        "  for i in range(len(test)):\n",
        "    src = torch.Tensor(test.iloc[i,:-1])\n",
        "    X = ens(src)\n",
        "    Ens_pred.append(X.squeeze(0).item())\n",
        "\n",
        "  Ens_pred = pd.DataFrame(Ens_pred)\n",
        "  Ens_pred.index = test.index\n",
        "  Ens_pred.columns = ['X-N-L-T GARCH']\n",
        "\n",
        "  XG_GARCH = ensemble(2)\n",
        "  NG_GARCH = ensemble(2)\n",
        "  LSTM_GARCH = ensemble(2)\n",
        "  TRANS_GARCH = ensemble(2)\n",
        "\n",
        "  XG_optimizer = torch.optim.Adam(XG_GARCH.parameters(), lr=lr, weight_decay=1e-5)\n",
        "  XG_scheduler = torch.optim.lr_scheduler.StepLR(XG_optimizer, 1.0, gamma=0.4)\n",
        "\n",
        "  NG_optimizer = torch.optim.Adam(NG_GARCH.parameters(), lr=lr, weight_decay=1e-5)\n",
        "  NG_scheduler = torch.optim.lr_scheduler.StepLR(NG_optimizer, 1.0, gamma=0.4)\n",
        "\n",
        "\n",
        "  LSTM_optimizer = torch.optim.Adam(LSTM_GARCH.parameters(), lr=lr, weight_decay=1e-5)\n",
        "  LSTM_scheduler = torch.optim.lr_scheduler.StepLR(LSTM_optimizer, 1.0, gamma=0.4)\n",
        "\n",
        "  TRANS_optimizer = torch.optim.Adam(TRANS_GARCH.parameters(), lr=lr, weight_decay=1e-5)\n",
        "  TRANS_scheduler = torch.optim.lr_scheduler.StepLR(TRANS_optimizer, 1.0, gamma=0.4)\n",
        "\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    train_ensemble(XG_GARCH,XG_optimizer,td[['XgBoost','GARCH','actual']][round(0.35*len(td)):round(0.85*len(td))])\n",
        "    train_ensemble(NG_GARCH,NG_optimizer,td[['NgBoost','GARCH','actual']][round(0.35*len(td)):round(0.85*len(td))])\n",
        "    train_ensemble(LSTM_GARCH,LSTM_optimizer,td[['LSTM','GARCH','actual']][round(0.35*len(td)):round(0.85*len(td))])\n",
        "    train_ensemble(TRANS_GARCH,TRANS_optimizer,td[['Transformer','GARCH','actual']][round(0.35*len(td)):round(0.85*len(td))])\n",
        "    XG_scheduler.step()\n",
        "    NG_scheduler.step()\n",
        "    LSTM_scheduler.step()\n",
        "    TRANS_scheduler.step()\n",
        "    print(f\"epoch n:  {epoch}\")\n",
        "\n",
        "  G_ens = []\n",
        "  test = td[round(0.85*len(td)):]\n",
        "  for i in range(len(test)):\n",
        "    y1 = XG_GARCH(torch.Tensor(test[['XgBoost','GARCH']].iloc[i])).squeeze(0).item()\n",
        "    y2 = NG_GARCH(torch.Tensor(test[['NgBoost','GARCH']].iloc[i])).squeeze(0).item()\n",
        "    y3 = LSTM_GARCH(torch.Tensor(test[['LSTM','GARCH']].iloc[i])).squeeze(0).item()\n",
        "    y4 = TRANS_GARCH(torch.Tensor(test[['Transformer','GARCH']].iloc[i])).squeeze(0).item()\n",
        "    G_ens.append([y1,y2,y3,y4])\n",
        "\n",
        "  G_ens = pd.DataFrame(G_ens)\n",
        "  G_ens.columns = ['XG GARCH','NG GARCH','LSTM GARCH','TRANS GARCH']\n",
        "  G_ens.index = test.index\n",
        "\n",
        "  final = td.merge(G_ens,left_index=True,right_index=True).merge(Ens_pred,left_index=True,right_index=True)\n",
        "  final.columns = [i + '_' + ticker[1:] + '_' + str(n) for i in final.columns]\n",
        "\n",
        "  return [XG_GARCH,NG_GARCH,LSTM_GARCH,TRANS_GARCH,ens_s,ens]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "folder_path = '/content/drive/MyDrive/Colab Notebooks/all_model_parameters'\n",
        "assets = [\"^RUT\",\"GC=F\",\"CL=F\"]\n",
        "for asset in assets:\n",
        "  for h in [60]:\n",
        "    mods = horizon_result(h,asset,15)\n",
        "\n",
        "    joblib.dump(mods[0], folder_path + '/xgb_EGARCH_' + asset + '_' + str(h) + '_model')\n",
        "    joblib.dump(mods[1], folder_path + '/ngb_EGARCH_' + asset + '_' + str(h) + '_model')\n",
        "    joblib.dump(mods[2], folder_path + '/lstm_EGARCH_' + asset + '_' + str(h) + '_model')\n",
        "    joblib.dump(mods[3], folder_path + '/transformer_EGARCH_' + asset + '_' + str(h) + '_model')\n",
        "    joblib.dump(mods[4], folder_path + '/X-N-L-T_' + asset + '_' + str(h) + '_model')\n",
        "    joblib.dump(mods[5], folder_path + '/X-N-L-T_EGARCH_' + asset + '_' + str(h) + '_model')\n"
      ],
      "metadata": {
        "id": "3vRyIHvizU2x"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "140BergWIsmNZ-Sh-1YZyPyATBlUaNHkK",
      "authorship_tag": "ABX9TyP+tCAZUQJi6fRdgoqxxwfI",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}